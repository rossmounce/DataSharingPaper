<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Timothée Poisot" />
  <meta name="author" content="Ross Mounce" />
  <meta name="author" content="Dominique Gravel" />
  <title>Moving toward a sustainable ecological science: don’t let data go to waste!</title>
  <link rel="stylesheet" href="pandoc-paper.css" type="text/css" />
</head>
<body>
<div id="header">
<h1 class="title">Moving toward a sustainable ecological science: don’t let data go to waste!</h1>
<h2 class="author">Timothée Poisot</h2>
<h2 class="author">Ross Mounce</h2>
<h2 class="author">Dominique Gravel</h2>
<h3 class="date">Nov. 2012</h3>
</div>
<h1 id="introduction">Introduction</h1>
<p>Claude Bernard <span class="citation">(Bernard 1864)</span> wrote that “art is <em>me</em>; science is <em>us</em>”. This sentence has two meanings. First, the altruism of scientists is worth more to Bernard than the self-indulgence of mid-nineteenth century Parisian art scene. Second, and we will keep this one in mind, creativity and insights come from individuals, but validation and rigor are reached through collective efforts, cross- validation, and peerage. Given enough time, the conclusions reached and validated by the efforts of many will take prominence over individualities, and this (as far as Bernard is concerned), is what science is about. With the technology available to a modern scientist, one should expect that the dissolution of <em>me</em> would be accelerated, and that several scientists should be able to cast a critical eye on data, and use this collective effort to draw robust conclusions.</p>
<p>In molecular evolution, there exists a large number of databases (<em>GenBank</em>, <em>EMBL</em>, <em>SwissProt</em>, and many more) in which information can be retrieved. This values (and allows) a new type of scientific research: building over the raw material of others, it is now possible to identify new phenomenon or evaluate the generality of previously studied ones. The job of these scientists is not to <em>make</em> data, neither to <em>stole</em> them, it’s rather to gather them and, most of all, look at them in a different way. This would not be possible, if not for the existence of public, free, online repositories. It’s sadly impossible to be as enthusiastic when looking at current practices in ecology. Apart from a few, non-specific initiatives (<em>DataDryad</em>), or small-scale initiatives which are not always properly maintained (<em>Interaction Web Networks Database</em>), there is no widespread data sharing culture among ecologists.</p>
<p>Yet in the recent years, there has been a strong signal that some organizations are ready to invest time and money in data sharing. For example, <em>DataONE</em> <span class="citation">(Reichman, Jones, and Schildhauer 2011)</span> is a large scale initiative, seeking to curate and make available observational data. We foresee that improving our data sharing practices will be an important endeavor in the coming years, and it increasing the awareness of the scientific community to these practices is a timely topic.</p>
<p>In this paper, using examples primarily taken from ecology and evolutionary biology, we will argue that improving our data sharing practices will improve both the quality of the science, and the reputation of the scientists. We illustrate how simple steps can be taken to greatly improve the situation, and how we can encourage the practice of data-sharing at different levels <span class="citation">(Whitlock et al. 2010)</span>, and data citation, to encourage and reward sharing. Our most important point is that through sharing more data, we will increase both the quality and visibility of the science we produce. We conclude this paper by showing that most of the technical aspects of data sharing can easily be mastered, meaning that data are ready to be liberated!</p>
<h1 id="why-we-ethically-must">Why we ethically must</h1>
<p>An important point to make is that data sharing is a moral obligation of sorts. In this part, we point out the ethical aspects of data sharing, both with regards to other scientists, funding agencies, and your own collaborators.</p>
<h2 id="data-acquisition-is-mostly-publicly-funded">Data acquisition is (mostly) publicly funded</h2>
<p>In contrasts with other fields such as energy, medicine, and pharmaceutical research, most ecology research is funded through public grants or charitably- funded programs. Or in other words, ecological research is enabled by the taxpayers. In some fields, most notably conservation biology, it is not uncommon that eco-volunteers participate in data gathering. For example, the French temporal survey of common birds <span class="citation">(Jiguet and Julliard 2006)</span>, which resulted in 29 publications in peer-reviewed journals, is fed entirely through the work of amateur ornithologists. Given the direct (participatory) or indirect (financial) involvement of the public in ecological data collection, it is not surprising that some funding agencies have implemented data availability policies. For example, the British <em>BBSRC</em> requires that “[p]ublicly-funded research data are a public good, produced in the public interest”, which “should be openly available to the maximum extent possible”. It then further adds that “[t]he value of data often depends on timeliness[;] it is expected that timely release would generally be no later than the release through publication of the main findings”. Similarly, <em>NERC</em> states that “[a]ll the environmental data held by the NERC Environmental Data Centres will normally be made openly available to any person or any organization who requests them.”. Sanctions are also put in place, as “[t]hose funded by NERC who do not meet these requirements risk having award payments withheld or becoming ineligible for future funding from NERC”. This perfectly mirrors one of the earliest debate about open-access: science which is made possible through public involvement must be made public. Publicly funded scientists, in most countries, are civil servants. Generating data is part of their job description, and there is no rational argument for which they should claim <em>property</em> of it. Claiming <em>paternity</em> of the data, as we discuss below, is a perfectly legitimate claim, but does not prevent sharing them.</p>
<h2 id="it-improves-reproducibility">It improves reproducibility</h2>
<p>Using journals to publish scientific information should not only serve the purpose of disseminating an interesting discussion of data; it should maximize the ability of other researchers to replicate, and thus both validate and expand, results. It is arguably a perversion of the <em>publish-or-perish</em> mentality, that we think only in terms of papers. Interestingly, although editors and referees are very careful about the way the <em>Materials &amp; Methods</em> sections of a paper are worded, it is extremely rare to receive any comment about the data availability. This can cause problems at all steps of the life of a paper. How can a paper describing a new method be adequately reviewed if data are not available? How can you be sure that you are correctly applying a method if you can’t reproduce the results? The movement of <em>reproducible research</em> <span class="citation">(Mesirov 2010)</span> advocates that a paper should be self-contained, <em>i.e.</em> be not only the text, but also the data, and the computer code to reproduce the figures. Even without going to such lengths, releasing data and computer code alongside a paper should be viewed as an ethical decision. Barnes <span class="citation">(Barnes 2010)</span> made the point that even though researchers are not professional programmers, computer code is good enough to be shared.</p>
<h2 id="it-will-clarify-authorship">It will clarify authorship</h2>
<p>It’s well accepted that the final for of a scientific article reflects the diversity of backgrounds and scientific sensibilities of its authors <span class="citation">(McGee 2011)</span>. Yet authorship, in the sense of deciding who gets to be listed as an author, and in which order, is still a key issue in several collaborations. Additionally, authorship deserves to be properly quantified <span class="citation">(Tscharntke et al. 2007)</span>, to reflect the amount of work done by each contributor. Too strict rules of authorship will not award proper recognition, and rules too open will grant undue credit. To some extent, journals attempted to qualify the work of each contributor by having special sections, indicating who wrote the paper, conceived the study, or contributed data or reagents. This is far from being anecdotal, as it allows for increased accountability <span class="citation">(Weltzin et al. 2006)</span>. By making dataset public and citable, the contribution of data will become less and less of a criteria for authorship. Because the datasets can be cited independently from their original paper, they will also contribute to the overall scientific impact of the researcher who generated them, thus allowing to name as authors only those who analysed the data.</p>
<h2 id="data-cost-money">Data cost money</h2>
<p>Gathering data, either in the lab or in the field, costs money, as it requires the acquisition and maintenance of equipments and reagents, in addition to salaries. In this perspective, generating new data when existing ones are available and could bring answers to a question is a wasteful practice. So as to avoid this, we need to have an easy way to find suitable data, which require thorough indexing. The large amount of hard to access data was dubbed ‘dark data’ <span class="citation">(Heidorn 2008)</span>. The fraction of data falling within this category is likely to increase. <span class="citation">(Wicherts et al. 2006)</span> surveyed the field of psychology, and showed that asking for the raw data often doesn’t result in a successful data sharing outcome, even after 6 months of repeated inquiries. Authors can claim to have ‘lost’ the data, can be extremely slow to reply, can ignore emails, the given contact email address may be invalid and difficult to find the ‘current’ contact address. Authors also die, and sadly this can result in the loss of valuable scientific data unless it has been accessibly and discoverably archived elsewhere. Ultimately, authors can also flat out refuse to give the data. The systematic practice of releasing data with non restrictive licences (some of which are explained in a later section), and associated with standardized meta-data, will help fight this effect. Overall, by making data easier to access, and better documenting them, we will decrease the flow of funding going into data gathering, and thus decrease the financial pressure on labs.</p>
<h1 id="which-benefits-it-will-bring-us">Which benefits it will bring us</h1>
<h2 id="a-proxy-to-your-science">A proxy to your science</h2>
<p>Datasets are a mean for people to get familiar with what you do. There are evidences showing that data availability improves reproducibility and adequate communication of your results <span class="citation">(Ince, Hatton, and Graham-Cumming 2012)</span>. Similarly, in some fields, releasing computer code under open source licenses <span class="citation">(Vandewalle 2012)</span> or sharing research data <span class="citation">(Piwowar, Day, and Fridsma 2007)</span> is associated with increased citation rates for your papers. Yet one of the argument often opposed by people reluctant to share their data is that they might risk loosing paternity of them. The previously cited analyses show that by <em>not</em> sharing data, we are exposed to a higher risk of our research being ignored, simply because people can not exploit it. By developping a culture of data sharing, and adequate citations of the datasets, the origin of the data (and thus their paternity) will be made clear. It seems that by keeping <em>property</em> over the data, there is a risk of them not being recognized, which will decrease a scientist’s impact.</p>
<h2 id="it-stimulates-collaboration-and-creativity">It stimulates collaboration and creativity</h2>
<h2 id="it-measures-your-productivity">It measures your productivity</h2>
<blockquote>
<p>A measure of your productivity that is increasingly being appreciated and encouraged by research funder agencies, as an example: the NSF (US) Grant Proposal Guidelines for 2013 have renamed the ‘Publications’ section to ‘Products’ specifically to make it clear that they apppreciate research products that “include, but are not limited to, publications, data sets, software, patents, and copyrights” (http://nsf.gov/pubs/policydocs/pappguide/nsf13001/gpg_sigchanges.jsp). Published datasets are now truly creditworthy, first class research objects (http://www.force11.org/white_paper) in the eyes of many funders.</p>
</blockquote>
<h1 id="how-we-technically-can">How we technically can</h1>
<p>In addition to the ethical and pragmatic arguments of the previous part, we engage here in a more technical reflexion about how we should include data sharing early in the studies, so as to generate data in a format allowing their re-usability. We also briefly discuss the different licensing options.</p>
<h2 id="data-representation">Data representation</h2>
<p>Except when they are deposited into large-scale databases, such as the ones we previously mentioned, data usually live (in various states of dormancy) on the hard drives of researchers. These data are usually formatted in the way where they were used to produce the few figures used in the published account, which is to say mostly as a spreadsheet, or a raw text file <span class="citation">(Akmon et al. 2011)</span>. Yet, more robust and sharing-friendly formats exists, which should be taken advantage of. For example, the <em>JavaScript Object Notation</em> <span class="citation">(Crockford 2006)</span> allows a context-rich representation of data, which can be based on templates. Building upon this format, a working group can put together a syntax to represent a given type of ecological data, then provide JSON templates for other people to release these data. In the ecological sciences, there are now publications outlets focused only on methodological papers (<em>Methods in Ecology and Evolution</em>, and to some extent <em>BMC Bioinformatics</em>), and several other journals have sections for methodological papers. JSON parsers exists for almost all languages (notably C, Python, R, Java), which means that different applications will be able to access the shared information. Under this perspective, it is possible to build local databases. As long as they respect the specification, groups only need to share the access to these databases, to enable all scientists to access the data. A “global” access can still be achieved by wrapping all of the local data sources, though an API, as detailed in the following section.</p>
<h2 id="database-linkage">Database linkage</h2>
<p>An important obstacle is that maintaining a global database requires funding on a scale which is orders of magnitude higher than what most grants will cover. The other solution, building on an increased use of strict data specification, is to link several local databases through APIs. In short, an API is an application stored on a server, which will offer several <em>methods</em>, each returning a <em>reply</em>. For example, a <em>method</em> can be “retrieve all datasets containing species A”, and the <em>reply</em> will be a list of datasets identifiers. If a particular data format is applied to more than one databases, it becomes possible to query them at once. Under this perspective, the origin of the data do not matter, because the API will return them in a standardized fashion. Each group implementing such a database can, in this situation, share the informations related to data access. Instead of putting the raw data on a data sharing platform (some of which are reviewed below), the authors will give informations about the study, and informations about where the data are stored, and how to access them.</p>
<h2 id="licensing-issues">Licensing issues</h2>
<p>Perhaps the point with which scientists will have the less familiarity is the licensing under which data should be made available. Fortunately, easy to understand, non-restrictive licenses exists, which are fitted to scientific output. The most well known family of them is the <em>Creative Commons</em> (CC). This family of licenses originated in the need to allow the free (as in speech, or as in beer) diffusion of cultural, artistic, and intellectual productions <span class="citation">(Lessig 2004)</span>, and it’s difficult to argue that scientific output do not fall within this category. <span class="citation">(Hrynaszkiewicz and Cockerill 2012)</span> proposes that factual data, on which no copyright can be applied, be released under the <em>CC-0 waiver</em>, which essentially removes all copyright on the data. This particular licence is imposed by <em>Dryad</em> (a data repository associated with, <em>e.g.</em>, <em>The American Naturalist</em>) and <em>fig<strong>share</strong></em> (though only for datasets). Another alternative is the <em>CC-BY</em> license, which allows use and reproduction of the data as long as the original source is explicitly mentioned (this licence is used for all non-data submissions in <em>fig<strong>share</strong></em>). Both of these licenses are in accordance with the Panton Principles <span class="citation">(Murray-Rust et al. 2010)</span>. The <em>Creative Commons</em> website offers an intuitive tool to choose a license (<code>http://creativecommons.org/choose/</code>), although we encourage scientists to be aware about the pitfalls associated with the most restrictive ones <span class="citation">(Hagedorn et al. 2011)</span>. In any case, although the <em>CC-0 waiver</em> makes it possible to use data without attribution, we encourage scientists not to do so, as we think it will effectively prevent the attribution of data, and thus decrease the usefulness of data sharing as a way to increase your (measured) scientific impact.</p>
<h1 id="how-it-should-be-encouraged">How it should be encouraged</h1>
<h2 id="the-role-of-journals">The role of journals</h2>
<p>Journals are in the best position to make things move <span class="citation">(Vision 2010)</span>, because a scientist career depend on getting its papers accepted. Although when possible, a bottom-up approach should always be preferred, editors have in their hand a formidable lever to modify our collective behavior. Some journals are now asking the authors to deposit their ecological data in a public repository <span class="citation">(Fairbairn 2011 ; Whitlock et al. 2010)</span>. This is mandatory for sequences in all journals (<em>GenBank</em>), and similar mandatory archiving of all data in TreeBase, DataDryad, or FigShare is becoming a common practice. The referees are, however, rarely asked to evaluate if the adequate data are released, and even more rarely given access to the data during the evaluation process. About this last point, an increased collaboration between journals and data sharing platforms, to allow referees to anonynously access the data, should be encouraged. In practice, authors are still free to release summary statistics instead of raw data, which allows to reproduce the paper, but not to confirm the validity of the approach.</p>
<p>Journal-led mandates cannot be the only solution used. When compliance with journal stipulations are retrospectively checked, even clinical trials data compliance <span class="citation">(Prayle, Hurley, and Smyth 2012)</span> and <em>GenBank</em> archiving of data are not universally adhered to, even in the ‘best’ journals of highest reputation <span class="citation">(Noor, Zimmerman, and Teeter 2006)</span>. Journals must take care that data archiving mandates are enforced and not just ‘rhetoric’, be it through increased editorial control, or by asking the referees to evaluate the data sharing plans. In addition, journals should implement incentives for authors to cite the datasets, and not just the paper to which they are attached. Strong limitations on the number of references can currently impede this practice, as it will force authors to choose citations. In the context of meta-analyses, this can become especially problematic. The solution of having references part of the supplementary materials is not optimal either, as it comes with no assurance that they will be registered as a citation to the dataset, and will benefit from less exposure. To this effect, having an additional reference, as it will valorize the production of data as literature items.</p>
<h2 id="the-role-of-funding-agencies">The role of funding agencies</h2>
<h1 id="conclusion">Conclusion</h1>
<p>In the last two years, there were an important number of media outbursts, and public indignation, about the role of science and scientific conduct, which may all have been avoided if the practice of putting data publicly online was widespread. The so-called <em>climategate</em> <span class="citation">(Jasanoff 2010)</span> could have been largely averted if all data were made public in the earlier days of the affair, as it was later clearly demonstrated that the apparent lack of transparency eroded public trust in scientists <span class="citation">(Leiserowitz et al. 2010 ; Ravetz 2011)</span>. Even more recently, the controversy over a study on the carcinogenicity of GM maize <span class="citation">(Séralini et al. 2012)</span> was thickened by the refusal of both sides (Monsanto and the French research group) to release the full data, in addition to many undisclosed conflicts of interests <span class="citation">(Meldolesi 2012)</span>.</p>
<p>When journal editors started publicly discussing the matter, they called this <em>data archiving</em> <span class="citation">(Fairbairn 2011 ; Whitlock et al. 2010)</span>. We would exhort other scientists not to use this expression. Data <em>archiving</em> evocate cardboard boxes, in which data are put to collect some dust. Whether this happens in the hard-drive of a scientist or in a well-maintained repository only differs in the fact that the later solution comes with a DOI. We think that the process or making data available should be called in a way which reflects its objectives: <em>data sharing</em>. We have the technology in place to give data a second life, in which the scientific community can appropriate them, recognize the paternity of those who generated them, and acknowledge this through citations. Data are all we care about. They make our papers possible. They bring answers to our questions, and much better, questions to our answers. After serving us so well, they deserve better than to be <em>archived</em>.</p>
<p>Making data sharing the rule rather than the expection should be encouraged. Yet it does not mean that we should look down on the generation of new data. <strong>Continue this §</strong>.</p>
<p><strong>Acknowledgments</strong>: We thank Karthik Ram for offering us the opportunity to write this paper, and many people who gave feedback during the writing. This paper was developed in an open <em>GitHub</em> repository (<code>https://github.com/tpoisot/DataSharingPaper</code>), and is archived on <em>fig<strong>share</strong></em>. TP is a <em>fig<strong>share</strong></em> advisor. TP was funded by a FQRNT-MELS post-doctoral scholarship.</p>
<h1 id="references">References</h1>
<p>Akmon, Dharma, Ann Zimmerman, Morgan Daniels, and Margaret Hedstrom. 2011. “The application of archival concepts to a data-intensive environment: working with scientists to understand data management and preservation needs.” <em>Archival Science</em> 11: 329–348. doi:10.1007/s10502-011-9151-4.</p>
<p>Barnes, Nick. 2010. “Publish your computer code: it is good enough.” <em>Nature</em> 467: 753. doi:10.1038/467753a.</p>
<p>Bernard, C. 1864. <em>Introduction à l’étude de la médecine expérimentale</em>.</p>
<p>Crockford, Douglas. 2006. “The application/json Media Type for JavaScript Object Notation (JSON).” <a href="http://tools.ietf.org/html/rfc4627" title="http://tools.ietf.org/html/rfc4627">http://tools.ietf.org/html/rfc4627</a>.</p>
<p>Fairbairn, Daphne J. 2011. “The advent of mandatory data archiving.” <em>Evolution</em> 65: 1–2. doi:10.1111/j.1558-5646.2010.01182.x.</p>
<p>Hagedorn, Gregor, Daniel Mietchen, Robert Morris, Donat Agosti, Lyubomir Penev, Walter Berendsohn, and Donald Hobern. 2011. “Creative Commons licenses and the non-commercial condition: Implications for the re-use of biodiversity information.” <em>ZooKeys</em> 150: 127–149.</p>
<p>Heidorn, P. Bryan. 2008. “Shedding Light on the Dark Data in the Long Tail of Science.” <em>Library Trends</em> 57: 280–299. doi:10.1353/lib.0.0036.</p>
<p>Hrynaszkiewicz, Iain, and Matthew Cockerill. 2012. “Open by default: a proposed copyright license and waiver agreement for open access research and data in peer-reviewed journals.” <em>BMC Research Notes</em> 5: 494.</p>
<p>Ince, Darrel C., Leslie Hatton, and John Graham-Cumming. 2012. “The case for open computer programs.” <em>Nature</em> 482: 485–488. doi:10.1038/nature10836.</p>
<p>Jasanoff, Sheila. 2010. “Testing time for climate science.” <em>Science</em>.</p>
<p>Jiguet, F., and R. Julliard. 2006. “Suivi temporel des oiseaux communs. Bilan du programme STOC pour la France en 2005.” <em>Ornithos</em> 13: 158–165.</p>
<p>Leiserowitz, Anthony, Edward W. Maibach, Connie Roser-Renouf, Nicholas Smith, and Erica Dawson. 2010. “Climategate, public opinion, and the loss of trust.” <em>Social Science Research Network</em>.</p>
<p>Lessig, Lawrence. 2004. <em>Free culture: the nature and future of creativity</em>. New York: Penguin Press.</p>
<p>McGee, Glenn. 2011. “The Ethics of Authorship: Does It Take a Village to Write a Paper?.” <em>Science Careers</em>. <a href="http://sciencecareers.sciencemag.org/career_magazine/previous_issues/articles/2001_03_30/noDOI.2580479737297545632" title="http://sciencecareers.sciencemag.org/career_magazine/previous_issues/articles/2001_03_30/noDOI.2580479737297545632">http://sciencecareers.sciencemag.org/career_magazine/previous_issues/articles/2001_03_30/noDOI.2580479737297545632</a>.</p>
<p>Meldolesi, Anna. 2012. “Media leaps on French study claiming GM maize carcinogenicity.” <em>Nature Biotechnology</em> 30: 1018.</p>
<p>Mesirov, J. P. 2010. “Accessible Reproducible Research.” <em>Science</em> 327 (jan): 415–416. doi:10.1126/science.1179653. <a href="http://www.sciencemag.org/cgi/doi/10.1126/science.1179653" title="http://www.sciencemag.org/cgi/doi/10.1126/science.1179653">http://www.sciencemag.org/cgi/doi/10.1126/science.1179653</a>.</p>
<p>Murray-Rust, Peter, Cameron Neylon, Rufus Pollock, and John Wilbanks. 2010. “Panton Principles, Principles for open data in science.” <a href="http://pantonprinciples.org/" title="http://pantonprinciples.org/">http://pantonprinciples.org/</a>.</p>
<p>Noor, Mohamed A. F., Katherine J. Zimmerman, and Katherine C. Teeter. 2006. “Data Sharing: How Much Doesn’t Get Submitted to GenBank?.” <em>PLoS Biol</em> 4: 228.</p>
<p>Piwowar, Heather A., Roger S. Day, and Douglas B. Fridsma. 2007. “Sharing detailed research data is associated with increased citation rate.” <em>PloS one</em> 2: 308. doi:10.1371/journal.pone.0000308.</p>
<p>Prayle, Andrew P., Matthew N. Hurley, and Alan R. Smyth. 2012. “Compliance with mandatory reporting of clinical trial results on ClinicalTrials.gov: cross sectional study.” <em>BMJ</em> 344.</p>
<p>Ravetz, J. R. 2011. “’Climategate’ and the maturing of post-normal science.” <em>Futures</em>.</p>
<p>Reichman, O. J., Matthew B. Jones, and Mark P. Schildhauer. 2011. “Challenges and opportunities of open data in ecology.” <em>Science</em> 331: 703–5. doi:10.1126/science.1197962.</p>
<p>Séralini, Gilles-Eric, Emilie Clair, Robin Mesnage, Steeve Gress, Nicolas Defarge, Manuela Malatesta, Didier Hennequin, and Joël Spiroux de Vendômois. 2012. “Long term toxicity of a Roundup herbicide and a Roundup-tolerant genetically modified maize.” <em>Food and chemical toxicology</em> 50: 4221–31. doi:10.1016/j.fct.2012.08.005.</p>
<p>Tscharntke, Teja, Michael E. Hochberg, Tatyana A. Rand, Vincent H. Resh, and Jochen Krauss. 2007. “Author Sequence and Credit for Contributions in Multiauthored Publications.” <em>PLoS Biology</em> 5: 18. doi:10.1371/journal.pbio.0050018. <a href="http://biology.plosjournals.org/perlserv/?request=get-document&amp;doi=10.1371%2Fjournal.pbio.0050018" title="http://biology.plosjournals.org/perlserv/?request=get-document&amp;doi=10.1371%2Fjournal.pbio.0050018">http://biology.plosjournals.org/perlserv/?request=get-document&amp;doi=10.1371%2Fjournal.pbio.0050018</a>.</p>
<p>Vandewalle, Patrick. 2012. “Code Sharing is Associated with Research Impact in Image Processing.” <em>Computing in Science and Engineering</em>: 1–5.</p>
<p>Vision, Todd J. 2010. “Open Data and the Social Contract of Scientific Publishing.” <em>BioScience</em> 60: 330–331. doi:10.1525/bio.2010.60.5.2.</p>
<p>Weltzin, Jake F., R. Travis Belote, Leigh T. Williams, Jason K. Keller, and E. Cayenne Engel. 2006. “Authorship in ecology: attribution, accountability, and responsibility.” <em>Frontiers in Ecology and the Environment</em> 4: 435–441. doi:10.1890/1540-9295(2006)4[435:AIEAAA]2.0.CO;2. <a href="http://www.esajournals.org/doi/abs/10.1890/1540-9295%282006%294%5B435%3AAIEAAA%5D2.0.CO%3B2" title="http://www.esajournals.org/doi/abs/10.1890/1540-9295%282006%294%5B435%3AAIEAAA%5D2.0.CO%3B2">http://www.esajournals.org/doi/abs/10.1890/1540-9295%282006%294%5B435%3AAIEAAA%5D2.0.CO%3B2</a>.</p>
<p>Whitlock, Michael C., Mark A. McPeek, Mark D. Rausher, Loren Rieseberg, and Allen J. Moore. 2010. “Data archiving.” <em>The American naturalist</em> 175: 145–6. doi:10.1086/650340.</p>
<p>Wicherts, Jelte M., Denny Borsboom, Judith Kats, and Dylan Molenaar. 2006. “The poor availability of psychological research data for reanalysis.” <em>American Psychologist</em> 61: 726–728.</p>
</body>
</html>
